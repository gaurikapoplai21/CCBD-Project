{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "e2e ASR homework v1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gaurikapoplai21/CCBD-Project/blob/master/e2e_ASR_homework_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qncY3FktdgMI"
      },
      "source": [
        "# EN. 601.467/667 Introduction to Human Language Technology\n",
        "# End-to-End Speech Recognition Toolkit\n",
        "**Deadline 16, October, 2019**\n",
        "\n",
        "- We use [ESPnet](https://github.com/espnet/espnet)\n",
        "- This homework is designed to have an overview experience of end-to-end speech recognition. Various contents are not covered in the lecture due to the limitation of time.\n",
        "- Adapted from the [Interspeech 2019 tutorial materials](https://github.com/espnet/interspeech2019-tutorial).\n",
        "  - Special thanks to Shigeki Karita and Tomoki Hayashi.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSbPo3PMxBKa",
        "colab_type": "text"
      },
      "source": [
        "## Google colaboratory\n",
        "\n",
        "Before getting started, get familiar with google colaboratory:\n",
        "https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "- Online Jupyter notebook environment\n",
        "    - Can run python codes\n",
        "    - Can also run linux command with ! mark\n",
        "    - Can use a signal GPU (K80)\n",
        "- What you need to use\n",
        "    - Internet connection\n",
        "    - Google account\n",
        "    - Chrome browser (recommended)\n",
        "\n",
        "This is a neat python environment that works in the cloud and does not require you to\n",
        "set up anything on your personal machine\n",
        "(it also has some built-in IDE features that make writing code easier).\n",
        "Moreover, it allows you to copy any existing colaboratory file, alter it and share\n",
        "with other people. In this homework, we will ask you to copy current colaboraty,\n",
        "complete all the tasks and share your colaboratory notebook with us so\n",
        "that we can grade it.\n",
        "\n",
        "## Submission\n",
        "\n",
        "Before you start working on this homework do the following steps:\n",
        "\n",
        "1. Press __File > Save a copy in Drive...__ tab. This will allow you to have your own copy and change it.\n",
        "2. Follow all the steps in this colaboratory file and write/change/uncomment code as necessary.\n",
        "3. Do not forget to occasionally press __File > Save__ tab to save your progress.\n",
        "4. After all the changes are done and progress is saved press __Share__ button (top right corner of the page), press __get shareable link__ and make sure you have the option __Anyone with the link can view__ selected.\n",
        "5. Paste the link into your submission pdf file so that we can view it and grade.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAOHL0Rq-6Hm"
      },
      "source": [
        "# 1. Overview\n",
        "\n",
        "ESPnet provides **bash recipes and python library** for speech processing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SlGI71D4-6Hu"
      },
      "source": [
        "## 1.1 Python library overview\n",
        "\n",
        "- Python 3.6+\n",
        "- Uses the following neural network libraries\n",
        "  - PyTorch\n",
        "  - Chainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "USfjVUT8-6Hy"
      },
      "source": [
        "## 1.2 Bash recipe overview\n",
        "\n",
        "ESPnet supports total 34+ ASR and other speech processing tasks including\n",
        "\n",
        "- Multilingual ASR: en, zh, ja, etc\n",
        "- Noise robust and far-field ASR\n",
        "- Multichannel ASR: joint training with speech enhancement\n",
        "- Speech Translation: transfer learning from ASR and MT\n",
        "\n",
        "For more detail:\n",
        "https://github.com/espnet/espnet/tree/master/egs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lcQ6iKES-6H0"
      },
      "source": [
        "## 1.3 ASR Performance\n",
        "\n",
        "On free corpora, ESPnet achieved:\n",
        "\n",
        "- Aishell (zh): CER test: 6.7%\n",
        "- Common Voice (en): WER test: 2.3%\n",
        "- LibriSpeech (en): WER test-clean: 2.6%, test-other 5.7%\n",
        "- TED-LIUM2 (en): WER test: 8.1%\n",
        "\n",
        "**Pretrained models are available**\n",
        "\n",
        "https://github.com/espnet/espnet#asr-results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P1BzhOOn-6H4"
      },
      "source": [
        "# 2. Installation\n",
        "\n",
        "ESPnet depends on Kaldi ASR toolkit and Warp-CTC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BWdrn8p2_HxX"
      },
      "source": [
        "## 2.1 Installation (on Google colab)\n",
        "\n",
        "In Google colab, we can use pre-compiled binaries for faster startup (3 min):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mLxx6gVHwda6",
        "colab": {}
      },
      "source": [
        "# OS setup\n",
        "!cat /etc/os-release\n",
        "!apt-get install -qq bc tree sox\n",
        "\n",
        "# espnet setup\n",
        "!git clone --depth 5 https://github.com/espnet/espnet\n",
        "!pip install -q torch==1.1\n",
        "!cd espnet; pip install -q -e .\n",
        "\n",
        "# download pre-compiled warp-ctc and kaldi tools\n",
        "!espnet/utils/download_from_google_drive.sh \\\n",
        "    \"https://drive.google.com/open?id=13Y4tSygc8WtqzvAVGK_vRV9GlV7TRC0w\" espnet/tools tar.gz > /dev/null\n",
        "!cd espnet/tools/warp-ctc/pytorch_binding && \\\n",
        "    pip install -U dist/warpctc_pytorch-0.1.1-cp36-cp36m-linux_x86_64.whl\n",
        "\n",
        "# make dummy activate\n",
        "!mkdir -p espnet/tools/venv/bin && touch espnet/tools/venv/bin/activate\n",
        "!echo \"setup done.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_6pH9DX1hTLj"
      },
      "source": [
        "# 3. AN4 ASR experiments based on ESPnet\n",
        "\n",
        "- `espnet/egs/*/asr1/run.sh` is an out-of-the-box recipe\n",
        "- It reproduces our reported results\n",
        "\n",
        "![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages.png?raw=1)\n",
        "\n",
        "- **stage -1**: Download data if the data is available online.\n",
        "- **stage 0**: Prepare data to make kaldi-style data directory.\n",
        "- **stage 1**: Extract feature vector, calculate statistics, and normalize.\n",
        "- **stage 2**: Prepare a dictionary and make json files for training.\n",
        "- **stage 3**: Train the language model network.\n",
        "- **stage 4**: Train the E2E-ASR network.\n",
        "- **stage 5**: Decode speech data using the trained networks and perform scoring (provide the character error rate (CER), word error rate (WER), etc.).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A_ATcz0jnCcF"
      },
      "source": [
        "## 3.1 Kaldi-style directory structure\n",
        "\n",
        "Always we organize each recipe as `egs/xxx/asr1/run.sh`\n",
        "\n",
        "The most important directories:\n",
        "- `run.sh`: Main script of the recipe.\n",
        "- `cmd.sh`: Command configuration script to control how-to-run each job.\n",
        "- `path.sh`: Path configuration script. Basically, we do not need to touch.\n",
        "- `conf/`: Directory containing configuration files e.g.g.\n",
        "- `local/`: Directory containing the recipe-specific scripts e.g. data preparation.\n",
        "- `steps/` and `utils/`: Directory containing kaldi tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gsVAFRyNmr_h",
        "colab": {}
      },
      "source": [
        "# move on the recipe directory\n",
        "import os\n",
        "os.chdir(\"/content/espnet/egs/an4/asr1\")\n",
        "\n",
        "# check files\n",
        "!tree -L 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WkkVjeBM_PNo"
      },
      "source": [
        "\n",
        "## 3.2 Data preparation (Stage 0 - 2)\n",
        "\n",
        "For example, if you add `--stop-stage 2`, you can stop the script before neural network training.\n",
        "\n",
        "These stages perform FBANK speech feature extraction, normalization, and text formatting.\n",
        "\n",
        "![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_prep.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40PWi8wzY0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run stage -1 and then stop\n",
        "!./run.sh --stop_stage 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O88CpXHtHnZA"
      },
      "source": [
        "Now ready to start training!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xdguXokUwn7V"
      },
      "source": [
        "## 3.3 NN Training (Stage 3 - 4)\n",
        "\n",
        "You can configure NN training with `conf/train_xxx.yaml`\n",
        "\n",
        "![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_nn.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JwHNT4fuS4e",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.1 Run LM\n",
        "\n",
        "First, we train RNNLM for the AN4 data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-vNl2QcXwOGS",
        "colab": {}
      },
      "source": [
        "# it takes 2 minutes\n",
        "!./run.sh  --ngpu 1 --stage 3 --stop-stage 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvPSgyXU8XYJ",
        "colab_type": "text"
      },
      "source": [
        "After we finish the LM training, we can check the perplexity from the log file (`train.log`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFwcNkc28n8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the LM perplexity\n",
        "!cat exp/train_rnnlm_pytorch_lm_word100/train.log | grep perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dWeTSyq9g35",
        "colab_type": "text"
      },
      "source": [
        "Please check the `test perplexity` with the above command. The perplexity value should be around 14."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xxUnqdH44s1",
        "colab_type": "text"
      },
      "source": [
        "## 3.3.2 ASR NN training\n",
        "\n",
        "We train ASR NN for the AN4 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elmKPNufuS4T",
        "colab_type": "text"
      },
      "source": [
        "## ASR training config: RNN with Attention\n",
        "\n",
        "- you can check the default config in the following command\n",
        "- (optional) complete list of common options https://espnet.github.io/espnet/apis/espnet_bin.html#asr-train-py\n",
        "- (optional) complete list of model-specific options https://espnet.github.io/espnet/_modules/espnet/nets/pytorch_backend/e2e_asr.html#E2E.add_arguments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wmi7ggV4U_pv",
        "colab": {}
      },
      "source": [
        "!cat /content/espnet/egs/an4/asr1/conf/train_mtlalpha0.5.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTa5DlNK5L3j",
        "colab_type": "text"
      },
      "source": [
        "## Perform ASR NN training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUhOtZWN6f5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# WARNING: This code takes 5-6 minutes!\n",
        "!./run.sh  --ngpu 1 --stage 4 --stop-stage 4 --train-config conf/train_mtlalpha0.5.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvB9VGTIo8kA",
        "colab_type": "text"
      },
      "source": [
        "The training log file is stored in `exp/train_nodev_pytorch_train_mtlalpha0.5/train.log`.\n",
        "\n",
        "Let's first look at the validation result of the initial training with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g-PFzNrleqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!grep -e groundtruth -e prediction exp/train_nodev_pytorch_train_mtlalpha0.5/train.log \\\n",
        "  | sed -e 's/<eos>//g' -e 's/<space>/ /g' | head -n 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl4nZ2WjpMVt",
        "colab_type": "text"
      },
      "source": [
        "By comparing the groundtruth and prediction results, you could see that it did not produce meaningful results.\n",
        "\n",
        "Then, let's check the validation result of the final training with the following command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4W7sK9vnAba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!grep -e groundtruth -e prediction exp/train_nodev_pytorch_train_mtlalpha0.5/train.log \\\n",
        "  | sed -e 's/<eos>//g' -e 's/<space>/ /g' | tail -n 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-mSPs1tpw9e",
        "colab_type": "text"
      },
      "source": [
        "Compared with the initial result, the prediction is close to the groundtruth. Training seems to be going well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4nYAZsKuS4r",
        "colab_type": "text"
      },
      "source": [
        "You can also check the training and validation accuracies from `exp/train_nodev_pytorch_train_mtlalpha0.5/results/acc.png`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "YF9Nu7OeuS4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "from IPython.display import Image, display_png\n",
        "expdir = \"exp/train_nodev_pytorch_train_mtlalpha0.5/results/\"\n",
        "for name in [\"acc.png\"]:\n",
        "    print(name)\n",
        "    display_png(Image(expdir + name, width=500))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-RX1oryzt8Q",
        "colab_type": "text"
      },
      "source": [
        "Please confirm that both training and validation accuracies are improved with more epochs, but finally converged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2TA5RO6rVzlr"
      },
      "source": [
        "## 3.4 Decoding and evaluation (Stage 5)\n",
        "\n",
        "The last stage of ASR recipe\n",
        "\n",
        "![image.png](https://github.com/espnet/interspeech2019-tutorial/blob/master/notebooks/interspeech2019_asr/figs/stages_eval.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RLxqDSsXA3Xu"
      },
      "source": [
        "### decoding config\n",
        "- you can check the default config in the following command\n",
        "- (option) complete list of common options https://espnet.github.io/espnet/apis/espnet_bin.html#asr-recog-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NHtzMEczV3D1",
        "colab": {}
      },
      "source": [
        "!cat conf/decode_ctcweight0.5.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OHoUE27XZ3u",
        "colab_type": "text"
      },
      "source": [
        "### Language model weight\n",
        "- `lm-weight:` language model (LM) weight $\\lambda$\n",
        "\n",
        "- without LM\n",
        "\n",
        "  $\\hat{W} = \\arg \\max _{W} \\log p(W|O) = \\arg \\max _{W}  \\log p_{\\text{e2e}}(W|O)$\n",
        "\n",
        "\n",
        "- with LM\n",
        "\n",
        "  $\\hat{W} = \\arg \\max _{W} \\log p(W|O) = \\arg \\max _{W}  (\\log p_{\\text{e2e}}(W|O) + \\lambda \\log p_{\\text{lm}}(W))$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgNLfRf6uS40",
        "colab_type": "text"
      },
      "source": [
        "### Run ASR decoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R-NBPjKeRGdF",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "# WARNING: This code takes 6 minutes!\n",
        "# Only recognize the test set\n",
        "!sed -i.bak -e's/recog_set=\"train_dev test\"/recog_set=\"test\"/' run.sh\n",
        "# run the actual recognition script\n",
        "!./run.sh --stage 5 --decode-config conf/decode_ctcweight0.5.yaml --train-config conf/train_mtlalpha0.5.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q--YOTde-6Jf"
      },
      "source": [
        "You can get the Character Error Rate (CER) by checking the `Err` column in the last line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WInK0cGPuS6A",
        "colab_type": "text"
      },
      "source": [
        "## 4. Compare the result w/ and w/o language model (main homework)\n",
        "\n",
        "1. modify the decoding config file by the `sed` command and set `lm-weight` to `{0.0, 0.1, 0.2, 0.3}` and rename the config appropriately\n",
        "2. perform recognition\n",
        "\n",
        "The following is an example of how to change the language model weight\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4PnchKa1bu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sed -e 's/lm-weight: 1.0/lm-weight: 0.1/' conf/decode_ctcweight0.5.yaml | tee conf/decode_ctcweight0.5_lmweight0.1.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JqYVwcZ14Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!./run.sh --stage 5 --decode-config conf/decode_ctcweight0.5_lmweight0.1.yaml --train-config conf/train_mtlalpha0.5.yaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLiGcK1VxrTI",
        "colab_type": "text"
      },
      "source": [
        "**Important Note**: \n",
        "- Unfortunately, the result would be changing with different Google colab images. Please finish the above experiments in a half day (otherwise it may use a different image and results would not be consistent)"
      ]
    }
  ]
}